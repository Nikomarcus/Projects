{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "nltk.data.path.append('/Users/naveenverma/nltk_data')\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# A function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# A function for tokenizing and further processing each message\n",
    "def tokenize_message(message):\n",
    "    # Tokenize the message\n",
    "    tokens = word_tokenize(message)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Stem the tokens\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-4, reg_param=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.reg_param = reg_param\n",
    "        self.theta = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _loss(self, X, y):\n",
    "        z = np.dot(X, self.theta)\n",
    "        h = self._sigmoid(z)\n",
    "        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        # Regularization term\n",
    "        reg_term = self.reg_param * np.sum(self.theta[1:] ** 2)\n",
    "        loss += reg_term\n",
    "        return loss\n",
    "\n",
    "    def _gradient(self, X, y):\n",
    "        z = np.dot(X, self.theta)\n",
    "        h = self._sigmoid(z)\n",
    "        gradient = np.dot(X.T, (h - y)) / len(y)\n",
    "        # Regularization term gradient\n",
    "        reg_gradient = self.reg_param * np.concatenate(([0], self.theta[1:] * 2))\n",
    "        gradient += reg_gradient\n",
    "        return gradient\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        prev_loss = float('inf')\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            gradient = self._gradient(X, y)\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "\n",
    "            loss = self._loss(X, y)\n",
    "            if abs(prev_loss - loss) < self.tol:\n",
    "                break\n",
    "            prev_loss = loss\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        z = np.dot(X, self.theta)\n",
    "        proba = self._sigmoid(z)\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        proba = self.predict_proba(X)\n",
    "        predictions = (proba >= threshold).astype(int)\n",
    "        return predictions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                            Message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "Train set size: 3343\n",
      "Test set size: 1114\n",
      "Validation set size: 1115\n"
     ]
    }
   ],
   "source": [
    "# Read the text file into a DataFrame\n",
    "df = pd.read_csv('/Users/naveenverma/Desktop/NewStart/Dataset/a1-data/SMSSpamCollection', sep='\\t', header=None, names=['Label', 'Message'])\n",
    "# Data table\n",
    "print(df.head())\n",
    "\n",
    "# Apply preprocessing and tokenization to each message\n",
    "df['Processed_Message'] = df['Message'].apply(preprocess_text)\n",
    "df['Tokenized_Message'] = df['Processed_Message'].apply(tokenize_message)\n",
    "\n",
    "# Convert the text tokens back to strings\n",
    "df['Tokenized_Message'] = df['Tokenized_Message'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Split the dataset into train, test, and validation sets\n",
    "train_size = int(0.6 * len(df))\n",
    "test_size = int(0.2 * len(df))\n",
    "\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:train_size+test_size]\n",
    "val_data = df[train_size+test_size:]\n",
    "\n",
    "# Reset index for each dataset\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the sizes of the resulting sets\n",
    "print(\"Train set size:\", len(train_data))\n",
    "print(\"Test set size:\", len(test_data))\n",
    "print(\"Validation set size:\", len(val_data))\n",
    "\n",
    "# Prepare data for training\n",
    "X_train = train_data['Tokenized_Message'].values\n",
    "y_train = train_data['Label'].map({'ham': 0, 'spam': 1}).values\n",
    "\n",
    "X_test = test_data['Tokenized_Message'].values\n",
    "y_test = test_data['Label'].map({'ham': 0, 'spam': 1}).values\n",
    "\n",
    "X_val = val_data['Tokenized_Message'].values\n",
    "y_val = val_data['Label'].map({'ham': 0, 'spam': 1}).values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Class distribution\n",
    "import matplotlib.pyplot as plt\n",
    "df['Label'].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "def calculate_metrics(true_labels, predicted_labels):\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(true_labels == predicted_labels)\n",
    "\n",
    "    # Calculate true positives, true negatives, false positives, false negatives\n",
    "    tp = np.sum((true_labels == 0) & (predicted_labels == 0))\n",
    "    tn = np.sum((true_labels == 0) & (predicted_labels == 1))\n",
    "    fp = np.sum((true_labels == 1) & (predicted_labels == 0))\n",
    "    fn = np.sum((true_labels == 1) & (predicted_labels == 1))\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8599640933572711\n",
      "Precision: 0.8599640933572711\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9247104247104247\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "# For training, we need to convert the text data into numerical features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "model.fit(X_train_tfidf.toarray(), y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "predictions = model.predict(X_test_tfidf.toarray())\n",
    "\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "predictions = model.predict(X_test_tfidf.toarray())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy, precision, recall, f1_score = calculate_metrics(y_test, predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class SGDLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-4, reg_param=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.reg_param = reg_param\n",
    "        self.theta = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _loss(self, X, y):\n",
    "        z = np.dot(X, self.theta)\n",
    "        h = self._sigmoid(z)\n",
    "        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        # Regularization term\n",
    "        reg_term = self.reg_param * np.sum(self.theta[1:] ** 2)\n",
    "        loss += reg_term\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        prev_loss = float('inf')\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            # Shuffle the data\n",
    "            indices = np.random.permutation(len(X))\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for i in range(len(X)):\n",
    "                xi = X_shuffled[i]\n",
    "                yi = y_shuffled[i]\n",
    "                gradient = self._gradient(xi, yi)\n",
    "                self.theta -= self.learning_rate * gradient\n",
    "\n",
    "            loss = self._loss(X, y)\n",
    "            if abs(prev_loss - loss) < self.tol:\n",
    "                break\n",
    "            prev_loss = loss\n",
    "\n",
    "    def _gradient(self, xi, yi):\n",
    "        z = np.dot(xi, self.theta)\n",
    "        h = self._sigmoid(z)\n",
    "        gradient = np.dot(xi.T, (h - yi))\n",
    "        # Regularization term gradient\n",
    "        reg_gradient = self.reg_param * np.concatenate(([0], self.theta[1:] * 2))\n",
    "        gradient += reg_gradient\n",
    "        return gradient / len(yi) if hasattr(yi, \"__len__\") else gradient\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]  # add intercept term\n",
    "        z = np.dot(X, self.theta)\n",
    "        proba = self._sigmoid(z)\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        proba = self.predict_proba(X)\n",
    "        predictions = (proba >= threshold).astype(int)\n",
    "        return predictions\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8599640933572711\n",
      "Precision: 0.8599640933572711\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9247104247104247\n"
     ]
    }
   ],
   "source": [
    "sgd_classifier = SGDLogisticRegression(learning_rate=0.01, max_iter=1000, tol=1e-4)\n",
    "\n",
    "# Fit the model to the training data\n",
    "sgd_classifier.fit(X_train_tfidf.toarray(), y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = sgd_classifier.predict(X_test_tfidf.toarray())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy, precision, recall, f1_score = calculate_metrics(y_test, predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class MiniBatchLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-4, batch_size=32, random_state=None, reg_param=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.reg_param = reg_param\n",
    "        self.theta = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _initialize_weights(self, n_features):\n",
    "        np.random.seed(self.random_state)\n",
    "        self.theta = np.random.randn(n_features)\n",
    "\n",
    "    def _compute_gradient(self, X, y):\n",
    "        z = np.dot(X, self.theta)\n",
    "        h = self._sigmoid(z)\n",
    "        gradient = np.dot(X.T, (h - y)) / len(y)\n",
    "        # Regularization term gradient\n",
    "        reg_gradient = self.reg_param * np.concatenate(([0], self.theta[1:] * 2))\n",
    "        gradient += reg_gradient\n",
    "        return gradient\n",
    "\n",
    "    def _compute_loss(self, X, y):\n",
    "        z = np.dot(X, self.theta)\n",
    "        h = self._sigmoid(z)\n",
    "        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        # Regularization term\n",
    "        reg_term = self.reg_param * np.sum(self.theta[1:] ** 2)\n",
    "        loss += reg_term\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._initialize_weights(n_features)\n",
    "        prev_loss = float('inf')\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            X_shuffled, y_shuffled = self._shuffle_data(X, y)\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "                gradient = self._compute_gradient(X_batch, y_batch)\n",
    "                self.theta -= self.learning_rate * gradient\n",
    "\n",
    "            loss = self._compute_loss(X_shuffled, y_shuffled)\n",
    "            if abs(prev_loss - loss) < self.tol:\n",
    "                break\n",
    "            prev_loss = loss\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = np.dot(X, self.theta)\n",
    "        proba = self._sigmoid(z)\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        proba = self.predict_proba(X)\n",
    "        predictions = (proba >= threshold).astype(int)\n",
    "        return predictions\n",
    "\n",
    "    def _shuffle_data(self, X, y):\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        return X[indices], y[indices]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9210053859964094\n",
      "Precision: 0.9239766081871345\n",
      "Recall: 0.9239766081871345\n",
      "F1 Score: 0.9239766081871345\n"
     ]
    }
   ],
   "source": [
    "mini_batch_classifier = MiniBatchLogisticRegression(learning_rate=0.01, max_iter=1000, tol=1e-4, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "mini_batch_classifier.fit(X_train_tfidf.toarray(), y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = mini_batch_classifier.predict(X_test_tfidf.toarray())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy, precision, recall, f1_score = calculate_metrics(y_test, predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.001\n",
      "Test Set Metrics:\n",
      "Accuracy: 0.8699551569506726\n",
      "Precision: 0.8699551569506726\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9304556354916067\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation set\n",
    "X_val_tfidf = tfidf_vectorizer.transform(val_data['Tokenized_Message'].values)\n",
    "\n",
    "\n",
    "# Define a list of regularization parameter values to try\n",
    "lambda_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "best_lambda = None\n",
    "best_f1_score = -1  # Initialize with a value that will definitely be lower than any actual F1 score\n",
    "\n",
    "# Loop over lambda values\n",
    "for reg_param in lambda_values:\n",
    "    # Train the model\n",
    "    model = LogisticRegression(reg_param=reg_param)\n",
    "    model.fit(X_train_tfidf.toarray(), y_train)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    predictions = model.predict(X_val_tfidf.toarray())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    _, _, _, f1_score = calculate_metrics(y_val, predictions)\n",
    "\n",
    "    # Check if this lambda value gives better F1 score\n",
    "    if f1_score > best_f1_score:\n",
    "        best_lambda = reg_param\n",
    "        best_f1_score = f1_score\n",
    "\n",
    "# Train the model with the best lambda on the entire training set\n",
    "best_model = LogisticRegression(reg_param=best_lambda)\n",
    "best_model.fit(X_train_tfidf.toarray(), y_train)\n",
    "predictions = best_model.predict(X_val_tfidf.toarray())\n",
    "# Evaluate on the validation set\n",
    "accuracy, precision, recall, f1_score = calculate_metrics(y_val, predictions)\n",
    "\n",
    "print(\"Best lambda:\", best_lambda)\n",
    "print(\"Test Set Metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As compare to my Logistic Regression and Stochastic GD, mini batch logistic regression performed well because mini batch gradient descent combines the advantage of both GD and SGD.\n",
    "1 - It is efficient.\n",
    "2 - It tries to avoid local minima by updating the parameters on a small subset of data."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

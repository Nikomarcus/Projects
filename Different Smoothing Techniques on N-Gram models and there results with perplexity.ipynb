{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def print_line(*args):\n",
    "    \"\"\" Inline print and go to the begining of line\n",
    "    \"\"\"\n",
    "    args1 = [str(arg) for arg in args]\n",
    "    str_ = ' '.join(args1)\n",
    "    print('\\r' + str_, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices available:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all physical devices, including GPUs\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Physical devices available:\")\n",
    "for device in physical_devices:\n",
    "    print(device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union, Dict\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train sentences: 42068\n",
      "number of valid sentences: 3370\n",
      "number of test sentences: 3165\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "data_path = '/Users/naveenverma/Desktop/NewStart/Dataset/a3-data'\n",
    "\n",
    "train_sentences = open(os.path.join(data_path, 'train.txt')).readlines()\n",
    "valid_sentences = open(os.path.join(data_path, 'valid.txt')).readlines()\n",
    "test_sentences = open(os.path.join(data_path, 'input.txt')).readlines()\n",
    "print('number of train sentences:', len(train_sentences))\n",
    "print('number of valid sentences:', len(valid_sentences))\n",
    "print('number of test sentences:', len(test_sentences))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "\n",
    "    def apply(self, sentence: str) -> str:\n",
    "        \"\"\" Apply the preprocessing rules to the sentence\n",
    "        Args:\n",
    "            sentence: raw sentence\n",
    "        Returns:\n",
    "            sentence: clean sentence\n",
    "        \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.replace('<unk>', '')\n",
    "        if self.url:\n",
    "            sentence = Preprocessor.remove_url(sentence)\n",
    "        if self.punctuation:\n",
    "            sentence = Preprocessor.remove_punctuation(sentence)\n",
    "        if self.number:\n",
    "            sentence = Preprocessor.remove_number(sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(sentence: str) -> str:\n",
    "        \"\"\" Remove punctuations in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible punctuations\n",
    "        Returns:\n",
    "            sentence: sentence without punctuations\n",
    "        \"\"\"\n",
    "        sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(sentence: str) -> str:\n",
    "        \"\"\" Remove urls in text with re\n",
    "        Args:\n",
    "            sentence: sentence with possible urls\n",
    "        Returns:\n",
    "            sentence: sentence without urls\n",
    "        \"\"\"\n",
    "        sentence = re.sub(r'(https|http)?://(\\w|\\.|/|\\?|=|&|%)*\\b', ' ', sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_number(sentence: str) -> str:\n",
    "        \"\"\" Remove numbers in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible numbers\n",
    "        Returns:\n",
    "            sentence: sentence without numbers\n",
    "        \"\"\"\n",
    "        sentence = re.sub(r'\\d+', ' ', sentence)\n",
    "        return sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, sos_token='<s>', eos_token='</s>', pad_token='<pad>', unk_token='<unk>', mask_token='<mask>'):\n",
    "        # Special tokens.\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.mask_token = mask_token\n",
    "\n",
    "        self.vocab = { sos_token: 0, eos_token: 1, pad_token: 2, unk_token: 3, mask_token: 4 }  # token -> id\n",
    "        self.inverse_vocab = { 0: sos_token, 1: eos_token, 2: pad_token, 3: unk_token, 4: mask_token }  # id -> token\n",
    "        self.token_occurrence = { sos_token: 0, eos_token: 0, pad_token: 0, unk_token: 0, mask_token: 0 }  # token -> occurrence\n",
    "\n",
    "        self.preprocessor = Preprocessor()\n",
    "\n",
    "    @property\n",
    "    def sos_token_id(self):\n",
    "        \"\"\" Create a property method.\n",
    "            You can use self.sos_token_id or tokenizer.sos_token_id to get the id of the sos_token.\n",
    "        \"\"\"\n",
    "        return self.vocab[self.sos_token]\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self):\n",
    "        return self.vocab[self.eos_token]\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self.vocab[self.pad_token]\n",
    "\n",
    "    @property\n",
    "    def unk_token_id(self):\n",
    "        return self.vocab[self.unk_token]\n",
    "\n",
    "    @property\n",
    "    def mask_token_id(self):\n",
    "        return self.vocab[self.mask_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" A magic method that enable program to know the number of tokens by calling:\n",
    "            ```python\n",
    "            tokenizer = Tokenizer()\n",
    "            num_tokens = len(tokenizer)\n",
    "            ```\n",
    "        \"\"\"\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def fit(self, sentences: List[str]):\n",
    "        \"\"\" Fit the tokenizer using all sentences.\n",
    "        1. Tokenize the sentence by splitting with spaces.\n",
    "        2. Record the occurrence of all tokens\n",
    "        3. Construct the token to index (self.vocab) map and the inversed map (self.inverse_vocab) based on the occurrence. The token with a higher occurrence has the smaller index\n",
    "\n",
    "        Args:\n",
    "            sentences: All sentences in the dataset.\n",
    "        \"\"\"\n",
    "        n = len(sentences)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == n - 1:\n",
    "                print_line('Fitting Tokenizer:', (i + 1), '/', n)\n",
    "            tokens = self.preprocessor.apply(sentence.strip()).split()\n",
    "            if len(tokens) <= 1:\n",
    "                continue\n",
    "            for token in tokens:\n",
    "                if token == '<unk>':\n",
    "                    continue\n",
    "                self.token_occurrence[token] = self.token_occurrence.get(token, 0) + 1\n",
    "        print_line('\\n')\n",
    "\n",
    "        token_occurrence = sorted(self.token_occurrence.items(), key=lambda e: e[1], reverse=True)\n",
    "        for token, occurrence in token_occurrence[:-5]:\n",
    "            token_id = len(self.vocab)\n",
    "            self.vocab[token] = token_id\n",
    "            self.inverse_vocab[token_id] = token\n",
    "\n",
    "        print('The number of distinct tokens:', len(self.vocab))\n",
    "\n",
    "    def encode(self, sentences: List[str]) -> List[List[int]]:\n",
    "        \"\"\" Encode the sentences into token ids\n",
    "            Note: 1. if a token in a sentence does not exist in the fit encoder, we ignore it.\n",
    "                  2. If the number of tokens in a sentence is less than two, we ignore this sentence.\n",
    "                  3. Note that, for every sentence, we will add an sos_token, i.e., the id of <s> at the start of the sentence,\n",
    "                     and add an eos_token, i.e., the id of </s> at the end of the sentence.\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            sent_token_ids: A list of id list\n",
    "        \"\"\"\n",
    "        n = len(sentences)\n",
    "        sent_token_ids = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == n - 1:\n",
    "                print_line('Encoding with Tokenizer:', (i + 1), '/', n)\n",
    "            token_ids = []\n",
    "            tokens = self.preprocessor.apply(sentence.strip()).split()\n",
    "            for token in tokens:\n",
    "                if token == '<unk>':\n",
    "                    continue\n",
    "                if token in self.vocab:\n",
    "                    token_ids.append(self.vocab[token])\n",
    "            if len(token_ids) <= 1:\n",
    "                continue\n",
    "            token_ids = [self.sos_token_id] + token_ids + [self.eos_token_id]\n",
    "            sent_token_ids.append(token_ids)\n",
    "        print_line('\\n')\n",
    "        return sent_token_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Tokenizer: 2 / 2\r\n",
      "The number of distinct tokens: 44\n",
      "\n",
      "n : 2\n",
      "aer : 1\n",
      "banknote : 1\n",
      "berlitz : 1\n",
      "calloway : 1\n",
      "centrust : 1\n",
      "cluett : 1\n",
      "fromstein : 1\n",
      "gitano : 1\n",
      "guterman : 1\n",
      "\n",
      "Encoding with Tokenizer: 2 / 2\r\n",
      "\n",
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      " ['<s>', 'aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro', 'quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack', 'food', 'ssangyong', 'swapo', 'wachter', '</s>'] \n",
      "\n",
      " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
      " ['<s>', 'pierre', 'n', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov', 'n', '</s>'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(train_sentences[:2])\n",
    "print()\n",
    "\n",
    "token_occurrence = sorted(tokenizer.token_occurrence.items(), key=lambda e: e[1], reverse=True)\n",
    "for token, occurrence in token_occurrence[:10]:\n",
    "    print(token, ':', occurrence)\n",
    "print()\n",
    "sent_token_ids = tokenizer.encode(train_sentences[:2])\n",
    "print()\n",
    "for original_sentence, token_ids in zip(train_sentences[:2], sent_token_ids):\n",
    "    sentence = [tokenizer.inverse_vocab[token] for token in token_ids]\n",
    "    print(original_sentence, sentence, '\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Tokenizer: 42068 / 42068\r\n",
      "The number of distinct tokens: 9614\n",
      "Encoding with Tokenizer: 42068 / 42068\r\n",
      "Encoding with Tokenizer: 3370 / 3370\r\n",
      "Encoding with Tokenizer: 3165 / 3165\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(train_sentences)\n",
    "train_token_ids = tokenizer.encode(train_sentences)\n",
    "valid_token_ids = tokenizer.encode(valid_sentences)\n",
    "test_token_ids = tokenizer.encode(test_sentences)\n",
    "print(train_token_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_unigram_count(train_token_ids: List[List[int]]) -> Dict:\n",
    "    \"\"\" Calculate the occurrence of each token in the dataset.\n",
    "\n",
    "    Args:\n",
    "        train_token_ids: each element is a list of token ids\n",
    "    Return:\n",
    "        unigram_count: A map from token_id to occurrence\n",
    "    \"\"\"\n",
    "    unigram_count = defaultdict(int)\n",
    "    for sentence_token_ids in train_token_ids:\n",
    "        for token_id in sentence_token_ids:\n",
    "            unigram_count[token_id] += 1\n",
    "    return dict(unigram_count)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def get_bigram_count(train_token_ids: List[List[int]]) -> Dict[int, Dict]:\n",
    "    \"\"\" Calculate the occurrence of bigrams in the dataset.\n",
    "\n",
    "    Args:\n",
    "        train_token_ids: each element is a list of token ids\n",
    "    Return:\n",
    "        bigram_count: A map from token_id to next token occurrence. Key: token_id, value: Dict[token_id -> occurrence]\n",
    "                      For example, {\n",
    "                          5: { 10: 5, 20: 4 }\n",
    "                      } means (5, 10) occurs 5 times, (5, 20) occurs 4 times.\n",
    "    \"\"\"\n",
    "    bigram_count = defaultdict(lambda: defaultdict(int))\n",
    "    for sentence_token_ids in train_token_ids:\n",
    "        for i in range(len(sentence_token_ids) - 1):\n",
    "            current_token_id = sentence_token_ids[i]\n",
    "            next_token_id = sentence_token_ids[i + 1]\n",
    "            bigram_count[current_token_id][next_token_id] += 1\n",
    "    return dict(bigram_count)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "class BiGram:\n",
    "    def __init__(self, unigram_count, bigram_count):\n",
    "        self.unigram_count = unigram_count\n",
    "        self.bigram_count = bigram_count\n",
    "\n",
    "    def calc_prob(self, w1: int, w2: int) -> float:\n",
    "        \"\"\" Calculate the probability of p(w2 | w1) using the BiGram model.\n",
    "\n",
    "        Args:\n",
    "            w1, w2: current token and next token\n",
    "        Note:\n",
    "            if prob you calculated is 0, you should return 1e-5.\n",
    "        \"\"\"\n",
    "        # Calculate the probability of w2 given w1\n",
    "        if w1 in self.bigram_count and w2 in self.bigram_count[w1]:\n",
    "            count_w1_w2 = self.bigram_count[w1][w2]\n",
    "            count_w1 = self.unigram_count[w1]\n",
    "            if count_w1 != 0:\n",
    "                prob = count_w1_w2 / count_w1\n",
    "            else:\n",
    "                prob = 0\n",
    "        else:\n",
    "            prob = 0\n",
    "\n",
    "        # If prob is 0, return a small value instead to avoid zero probability\n",
    "        if prob == 0:\n",
    "            prob = 1e-5\n",
    "\n",
    "        return prob\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def power_law(x, a, b):\n",
    "    \"\"\" Power law to fit the number of occurrence\n",
    "    \"\"\"\n",
    "    return a * np.power(x, b)\n",
    "\n",
    "class GoodTuring(BiGram):\n",
    "    def __init__(self, unigram_count, bigram_count, threshold=100):\n",
    "        super().__init__(unigram_count, bigram_count)\n",
    "        self.threshold = threshold\n",
    "        self.bigram_Nc = self.calc_Nc()\n",
    "        self.bi_c_star, self.bi_N = self.smoothing(self.bigram_Nc)\n",
    "\n",
    "    def calc_Nc(self) -> Dict[int, Union[float, int]]:\n",
    "        bigram_Nc = {}\n",
    "        for _, next_counts in self.bigram_count.items():\n",
    "            for count in next_counts.values():\n",
    "                if count in bigram_Nc:\n",
    "                    bigram_Nc[count] += 1\n",
    "                else:\n",
    "                    bigram_Nc[count] = 1\n",
    "        self.replace_large_c(bigram_Nc)\n",
    "        return bigram_Nc\n",
    "\n",
    "    def replace_large_c(self, Nc):\n",
    "        x, y = zip(*sorted(Nc.items(), reverse=True))\n",
    "        popt, pcov = curve_fit(power_law, x, y, bounds=([0, -np.inf], [np.inf, 0]))\n",
    "        a, b = popt\n",
    "        max_count = max(Nc.keys())\n",
    "        for c in range(self.threshold + 1, max_count + 2):\n",
    "            Nc[c] = power_law(c, a, b)\n",
    "\n",
    "    def smoothing(self, Nc: Dict[int, Union[float, int]]) -> Tuple[Dict[int, float], float]:\n",
    "        c_star = {}\n",
    "        N = 0\n",
    "        max_count = max(Nc.keys())\n",
    "        for count, occurrence in Nc.items():\n",
    "            c_star[count] = (count + 1) * Nc.get(count + 1, 0) / Nc.get(count, 1)\n",
    "            N += count * occurrence\n",
    "        return c_star, N\n",
    "\n",
    "    def calc_prob(self, w1, w2):\n",
    "        prob = 0\n",
    "        if w1 in self.bigram_count and w2 in self.bigram_count[w1]:\n",
    "            count_w1_w2 = self.bigram_count[w1][w2]\n",
    "            if count_w1_w2 in self.bi_c_star:\n",
    "                smoothed_count = self.bi_c_star[count_w1_w2]\n",
    "                if w1 in self.unigram_count:\n",
    "                    count_w1 = self.unigram_count[w1]\n",
    "                    prob = smoothed_count / count_w1\n",
    "                else:\n",
    "                    prob = smoothed_count / self.bi_N\n",
    "        else:\n",
    "            if w1 in self.unigram_count:\n",
    "                count_w1 = self.unigram_count[w1]\n",
    "                prob = 1 / (count_w1 + 1)  # Add-one smoothing\n",
    "            else:\n",
    "                prob = 1 / len(self.unigram_count)  # Uniform distribution\n",
    "        return prob\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "class KneserNey(BiGram):\n",
    "    def __init__(self, unigram_count, bigram_count, d=0.75):\n",
    "        super().__init__(unigram_count, bigram_count)\n",
    "        self.d = d\n",
    "        self.lambda_ = self.calc_lambda()\n",
    "        self.p_continuation = self.calc_p_continuation()\n",
    "\n",
    "    def calc_lambda(self):\n",
    "        \"\"\" Calculate the Î»(w) \"\"\"\n",
    "        lambda_ = {}\n",
    "        for w1 in self.bigram_count:\n",
    "            lambda_[w1] = self.d * len(self.bigram_count[w1]) / self.unigram_count[w1]\n",
    "        return lambda_\n",
    "\n",
    "    def calc_p_continuation(self):\n",
    "        \"\"\" Calculate the p_continuation(w) \"\"\"\n",
    "        numerator = {}\n",
    "        for w1, next_counts in self.bigram_count.items():\n",
    "            for _ in next_counts:\n",
    "                numerator[_] = numerator.get(_, 0) + 1\n",
    "        denominator = len(numerator)\n",
    "        p_continuation = {}\n",
    "        for w, count in numerator.items():\n",
    "            p_continuation[w] = count / denominator\n",
    "        return p_continuation\n",
    "\n",
    "    def calc_prob(self, w1, w2):\n",
    "        \"\"\" Calculate the probability of p(w2 | w1) using the Kneser-Ney model. \"\"\"\n",
    "        c_w1_w2 = self.bigram_count[w1].get(w2, 0)\n",
    "        prob = max(c_w1_w2 - self.d, 0) / self.unigram_count[w1] + self.lambda_[w1] * self.p_continuation[w2]\n",
    "        return prob\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def perplexity(model, token_ids):\n",
    "    \"\"\" Calculate the perplexity score.\n",
    "\n",
    "    Args:\n",
    "        model: the model you want to evaluate (BiGram, GoodTuring, or KneserNey)\n",
    "        token_ids: a list of validation token_ids\n",
    "    Return:\n",
    "        perplexity: the perplexity of the model on texts\n",
    "    \"\"\"\n",
    "    log_probs = 0\n",
    "    n = len(token_ids)\n",
    "    n_words = 0\n",
    "    for i, tokens in enumerate(token_ids):\n",
    "        if i % 100 == 0 or i == n - 1:\n",
    "            print_line('Calculating perplexity:', (i + 1), '/', n)\n",
    "        # Calculate log probability of each token pair\n",
    "        for j in range(1, len(tokens)):\n",
    "            log_prob = math.log(model.calc_prob(tokens[j - 1], tokens[j]) + 1e-10)  # Add small value to avoid log(0)\n",
    "            log_probs += log_prob\n",
    "            n_words += 1\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perp = math.exp(-log_probs / n_words)\n",
    "    print('\\n')\n",
    "\n",
    "    return perp\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "unigram_count = get_unigram_count(train_token_ids)\n",
    "bigram_count = get_bigram_count(train_token_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating perplexity: 3352 / 3352\n",
      "\n",
      "The perplexity of Bigram is: 325.8347\n"
     ]
    }
   ],
   "source": [
    "bigram = BiGram(unigram_count, bigram_count)\n",
    "\n",
    "# Perplexity\n",
    "bigram_perplexity = perplexity(bigram, valid_token_ids)\n",
    "print(f'The perplexity of Bigram is: {bigram_perplexity:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating perplexity: 3352 / 3352\n",
      "\n",
      "The perplexity of Good Turing is: 95.1412\n"
     ]
    }
   ],
   "source": [
    "# Initialize the GoodTuring object\n",
    "gt = GoodTuring(unigram_count, bigram_count, threshold=100)\n",
    "\n",
    "gt_perplexity = perplexity(gt, valid_token_ids)\n",
    "\n",
    "# Print the perplexity value\n",
    "print(f'The perplexity of Good Turing is: {gt_perplexity:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating perplexity: 3352 / 3352\n",
      "\n",
      "The perplexity of Kneser-Ney is: 62.5908\n"
     ]
    }
   ],
   "source": [
    "kn = KneserNey(unigram_count, bigram_count, d=0.75)\n",
    "\n",
    "# Perplexity\n",
    "kn_perplexity = perplexity(kn, valid_token_ids)\n",
    "print(f'The perplexity of Kneser-Ney is: {kn_perplexity:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "def predict(model: 'BiGram', w1: int, vocab_size: int):\n",
    "    \"\"\" Predict the w2 with the highest probability given w1\n",
    "\n",
    "    Args:\n",
    "        model: A BiGram, GoodTuring, or KneserNey model that has the calc_prob function\n",
    "        w1: current word\n",
    "        vocab_size: the number of tokens in the vocabulary\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    highest_prob = 0\n",
    "    for w2 in range(1, vocab_size):\n",
    "        prob = model.calc_prob(w1, w2)\n",
    "        if prob > highest_prob:\n",
    "            highest_prob = prob\n",
    "            result = w2\n",
    "    return result\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharply falling stock prices do reduce consumer wealth damage business ____\n",
      "predicted last token: </s>\n",
      "---------------------------------------------\n",
      "but robert an official of the association said no ____\n",
      "predicted last token: longer\n",
      "---------------------------------------------\n",
      "it also has interests in military electronics and marine ____\n",
      "predicted last token: s\n",
      "---------------------------------------------\n",
      "first chicago since n has reduced its loans to such ____\n",
      "predicted last token: as\n",
      "---------------------------------------------\n",
      "david m jones vice president at g ____\n",
      "predicted last token: s\n",
      "---------------------------------------------\n",
      "the n stock specialist firms on the big board floor ____\n",
      "predicted last token: traders\n",
      "---------------------------------------------\n",
      "at the same time the business was hurt by ____\n",
      "predicted last token: the\n",
      "---------------------------------------------\n",
      "salomon will cover the warrants by buying sufficient shares or ____\n",
      "predicted last token: n\n",
      "---------------------------------------------\n",
      "in july southmark corp the dallas based real estate and financial ____\n",
      "predicted last token: officer\n",
      "---------------------------------------------\n",
      "he concluded his remarks by and at some ____\n",
      "predicted last token: of\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "indexes = np.random.choice(len(test_token_ids), 10, replace=False)\n",
    "for i in indexes:\n",
    "    token_ids = test_token_ids[i][1:-1]\n",
    "    print(' '.join([tokenizer.inverse_vocab[token_id] for token_id in token_ids]) + ' ____')\n",
    "    pred = predict(gt, token_ids[-1], vocab_size)\n",
    "    print(f'predicted last token: {tokenizer.inverse_vocab[pred]}')\n",
    "    print('---------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "        self.bias_input_hidden = np.zeros((1, hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "        self.bias_hidden_output = np.zeros((1, output_size))\n",
    "        self.cache_weights_input_hidden = np.zeros((input_size, hidden_size))\n",
    "        self.cache_bias_input_hidden = np.zeros((1, hidden_size))\n",
    "        self.cache_weights_hidden_output = np.zeros((hidden_size, output_size))\n",
    "        self.cache_bias_hidden_output = np.zeros((1, output_size))\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_input_hidden\n",
    "        self.hidden_output = self.relu(self.hidden_input)\n",
    "        output = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_hidden_output\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def relu_derivative(self, X):\n",
    "        return np.where(X > 0, 1, 0)\n",
    "\n",
    "    def softmax(self, X):\n",
    "        exp_values = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        error = output - y\n",
    "        d_weights_hidden_output = np.dot(self.hidden_output.T, error)\n",
    "        d_bias_hidden_output = np.sum(error, axis=0, keepdims=True)\n",
    "        d_hidden_input = np.dot(error, self.weights_hidden_output.T) * self.relu_derivative(self.hidden_input)\n",
    "        d_weights_input_hidden = np.dot(X.T, d_hidden_input)\n",
    "        d_bias_input_hidden = np.sum(d_hidden_input, axis=0, keepdims=True)\n",
    "\n",
    "        # AdaGrad update\n",
    "        self.cache_weights_hidden_output += d_weights_hidden_output ** 2\n",
    "        self.cache_bias_hidden_output += d_bias_hidden_output ** 2\n",
    "        self.cache_weights_input_hidden += d_weights_input_hidden ** 2\n",
    "        self.cache_bias_input_hidden += d_bias_input_hidden ** 2\n",
    "\n",
    "        self.weights_hidden_output -= learning_rate * d_weights_hidden_output / (np.sqrt(self.cache_weights_hidden_output) + self.epsilon)\n",
    "        self.bias_hidden_output -= learning_rate * d_bias_hidden_output / (np.sqrt(self.cache_bias_hidden_output) + self.epsilon)\n",
    "        self.weights_input_hidden -= learning_rate * d_weights_input_hidden / (np.sqrt(self.cache_weights_input_hidden) + self.epsilon)\n",
    "        self.bias_input_hidden -= learning_rate * d_bias_input_hidden / (np.sqrt(self.cache_bias_input_hidden) + self.epsilon)\n",
    "\n",
    "    def train(self, X, y, num_epochs, learning_rate, batch_size):\n",
    "        num_samples = X.shape[0]\n",
    "        for epoch in range(num_epochs):\n",
    "        # Shuffle the data\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "        # Minibatch gradient descent\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "                output = self.forward(X_batch)\n",
    "\n",
    "            # Backward pass\n",
    "                if y_batch.ndim == 1:\n",
    "                    y_one_hot = np.eye(self.weights_hidden_output.shape[1])[y_batch]\n",
    "                else:\n",
    "                    y_one_hot = y_batch\n",
    "                self.backward(X_batch, y_one_hot, output, learning_rate)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                text = preprocess_text(parts[1])\n",
    "                label = parts[0]\n",
    "                data.append((text, label))\n",
    "    return data\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5744626407369499\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "file_path = \"/Users/naveenverma/Desktop/NewStart/Dataset/a1-data/books.txt\"  # Provide the path to your data file\n",
    "data = load_and_preprocess_data(file_path)\n",
    "X = np.array([entry[0] for entry in data])\n",
    "y = np.array([entry[1] for entry in data])\n",
    "\n",
    "# Vectorize textual data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Convert categorical labels to numeric values\n",
    "label_mapping = {'Jane Austen': 0, 'Arthur Conan Doyle': 1, 'Fyodor Dostoyevsky': 2}\n",
    "y_numeric = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized.toarray(), y_numeric, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the MLP model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 32\n",
    "output_size = 3\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 8\n",
    "\n",
    "mlp = MLP(input_size, hidden_size, output_size)\n",
    "mlp.train(X_train, y_train, num_epochs, learning_rate, batch_size)\n",
    "\n",
    "# Forward pass on test data\n",
    "output_test = mlp.forward(X_test)\n",
    "predictions_test = np.argmax(output_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions_test == y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def create_model(input_size, hidden_size, output_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)),\n",
    "        tf.keras.layers.Dense(output_size)\n",
    "    ])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 4ms/step - accuracy: 0.5556 - loss: 1.0820 - val_accuracy: 0.5557 - val_loss: 1.0543\n",
      "Epoch 2/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.5672 - loss: 1.0456 - val_accuracy: 0.5557 - val_loss: 1.0311\n",
      "Epoch 3/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.5728 - loss: 1.0209 - val_accuracy: 0.5557 - val_loss: 1.0129\n",
      "Epoch 4/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.5686 - loss: 1.0043 - val_accuracy: 0.5557 - val_loss: 0.9975\n",
      "Epoch 5/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.5711 - loss: 0.9878 - val_accuracy: 0.5557 - val_loss: 0.9844\n",
      "Epoch 6/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.5669 - loss: 0.9757 - val_accuracy: 0.5557 - val_loss: 0.9733\n",
      "Epoch 7/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.5684 - loss: 0.9631 - val_accuracy: 0.5557 - val_loss: 0.9637\n",
      "Epoch 8/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.5670 - loss: 0.9539 - val_accuracy: 0.5557 - val_loss: 0.9555\n",
      "Epoch 9/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.5686 - loss: 0.9437 - val_accuracy: 0.5557 - val_loss: 0.9484\n",
      "Epoch 10/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.5662 - loss: 0.9393 - val_accuracy: 0.5557 - val_loss: 0.9421\n",
      "\u001B[1m123/123\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 719us/step - accuracy: 0.5765 - loss: 0.9321\n",
      "Accuracy: 0.5713919997215271\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 32\n",
    "output_size = 3\n",
    "\n",
    "model = create_model(input_size, hidden_size, output_size)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", test_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 5ms/step - accuracy: 0.6644 - loss: 0.8107 - val_accuracy: 0.8996 - val_loss: 0.3492\n",
      "Epoch 2/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - accuracy: 0.9310 - loss: 0.2564 - val_accuracy: 0.9207 - val_loss: 0.2324\n",
      "Epoch 3/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - accuracy: 0.9626 - loss: 0.1382 - val_accuracy: 0.9229 - val_loss: 0.2083\n",
      "Epoch 4/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 4ms/step - accuracy: 0.9682 - loss: 0.1016 - val_accuracy: 0.9207 - val_loss: 0.2041\n",
      "Epoch 5/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 4ms/step - accuracy: 0.9791 - loss: 0.0713 - val_accuracy: 0.9184 - val_loss: 0.2069\n",
      "Epoch 6/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 4ms/step - accuracy: 0.9807 - loss: 0.0593 - val_accuracy: 0.9178 - val_loss: 0.2115\n",
      "Epoch 7/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 4ms/step - accuracy: 0.9849 - loss: 0.0499 - val_accuracy: 0.9133 - val_loss: 0.2187\n",
      "Epoch 8/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 4ms/step - accuracy: 0.9876 - loss: 0.0425 - val_accuracy: 0.9107 - val_loss: 0.2308\n",
      "Epoch 9/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - accuracy: 0.9893 - loss: 0.0359 - val_accuracy: 0.9095 - val_loss: 0.2434\n",
      "Epoch 10/10\n",
      "\u001B[1m391/391\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 4ms/step - accuracy: 0.9883 - loss: 0.0339 - val_accuracy: 0.9098 - val_loss: 0.2535\n",
      "\u001B[1m123/123\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 701us/step - accuracy: 0.9130 - loss: 0.2599\n",
      "Accuracy: 0.9135107398033142\n"
     ]
    }
   ],
   "source": [
    "model = create_model(input_size, hidden_size, output_size)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", test_accuracy)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conclusion-\n",
    "1 - My custom model for MLP trained on our training data gave me 57% accuracy on the test set using AdaGrad optimizer, I also used tensorflow to train the model and it also gave 57% accuracy.\n",
    "2 - I used the adam optimizer also to cross-check the accuracy and I am getting 91% accuracy which is higher than our AdaGrad optimizer on the same data.\n",
    "3 - In this case, we can say that adam optimizer is a better optimization technique for this task than the AdaGrad optimizer."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
